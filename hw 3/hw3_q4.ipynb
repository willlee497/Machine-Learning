{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# CS 446 — Homework: Linear Regression\n",
        "# ==============================================================\n",
        "\n",
        "# In this assignment you will:\n",
        "# 1) Build a clean preprocessing pipeline (impute → one-hot → z-score).\n",
        "# 2) Implement OLS (normal equation) and Ridge (closed-form).\n",
        "# 3) Implement Lasso via ISTA (proximal gradient with soft-thresholding).\n",
        "# 4) Motivate log-transform via target skewness; evaluate with Duan's smearing.\n",
        "\n",
        "# Each section contains TODOs with short instructions."
      ],
      "metadata": {
        "id": "UbFb9F-_NBXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "import os\n",
        "\n",
        "# Config\n",
        "np.random.seed(42)\n",
        "SHOW_PLOTS = not os.environ.get('HIDE_PLOTS', False)"
      ],
      "metadata": {
        "id": "fR33nuNmNBUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Part 1: Data Preparation\n",
        "# ------------------------------------------------------\n",
        "# Load data & split\n",
        "\n",
        "housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
        "X = housing.data\n",
        "y = pd.to_numeric(housing.target, errors=\"coerce\").astype(np.float64).to_numpy()"
      ],
      "metadata": {
        "id": "7qzwthhENBRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO a(i) Split the data into train, val and test split with a 70/15/15 ratio.\n",
        "# You can use the train_test_split function to do this: train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "def split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Split X, y into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        X : Features (array or DataFrame)\n",
        "        y : Targets (array or Series)\n",
        "        train_size : Proportion for training set (default 0.7)\n",
        "        val_size   : Proportion for validation set (default 0.15)\n",
        "        test_size  : Proportion for test set (default 0.15)\n",
        "        random_state : Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test\n",
        "    \"\"\"\n",
        "    assert abs(train_size + val_size + test_size - 1.0) < 1e-8, \\\n",
        "        \"train_size + val_size + test_size must sum to 1.\"\n",
        "\n",
        "    # Return copies to avoid SettingWithCopy issues\n",
        "    return (X_train.copy(), X_val.copy(), X_test.copy(),\n",
        "            y_train.copy(), y_val.copy(), y_test.copy())"
      ],
      "metadata": {
        "id": "hPUj7F5KVLLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)"
      ],
      "metadata": {
        "id": "ADRM8GSOVP9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[Split] Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
        "\n",
        "numerical_features   = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "print(f\"[DTypes] Numerical: {len(numerical_features)} | Categorical: {len(categorical_features)}\")"
      ],
      "metadata": {
        "id": "KiKVsT1vODaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute (train stats only)\n",
        "#TODO a(ii) Impute numeric with train median and categorical with train mode. Use this train stat for val and test\n",
        "\n",
        "def impute_data(X_train, X_val, X_test, features, strategy=\"median\"):\n",
        "    \"\"\"\n",
        "    Impute missing values in train/val/test splits using train statistics.\n",
        "\n",
        "    Args:\n",
        "        X_train, X_val, X_test : DataFrames\n",
        "        features : list of feature names (columns to impute)\n",
        "        strategy : \"median\" for numerical, \"mode\" for categorical\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_val, X_test : copies with imputed values\n",
        "    \"\"\"\n",
        "    X_train, X_val, X_test = X_train.copy(), X_val.copy(), X_test.copy()\n",
        "\n",
        "    if strategy == \"median\":\n",
        "        #TODO a(ii) calculate meadian\n",
        "        pass\n",
        "    elif strategy == \"mode\":\n",
        "        #TODO a(ii) calculate mean\n",
        "    else:\n",
        "        raise ValueError(\"strategy must be 'median' or 'mode'\")\n",
        "\n",
        "    #TODO a(ii) use fillna\n",
        "    #X_train.loc[:, features] =\n",
        "    #X_val.loc[:,   features] =\n",
        "    #X_test.loc[:,  features] =\n",
        "\n",
        "    return X_train, X_val, X_test"
      ],
      "metadata": {
        "id": "49G1a8H1U5Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical imputation with median\n",
        "X_train, X_val, X_test = impute_data(X_train, X_val, X_test, numerical_features, strategy=\"median\")\n",
        "\n",
        "# Categorical imputation with mode\n",
        "if len(categorical_features) > 0:\n",
        "    X_train, X_val, X_test = impute_data(X_train, X_val, X_test, categorical_features, strategy=\"mode\")\n",
        "\n",
        "print(\"[Impute] Done (train medians/modes applied).\")"
      ],
      "metadata": {
        "id": "W8dzQO-LOGA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot + align\n",
        "if len(categorical_features) > 0:\n",
        "    X_train_cat = pd.get_dummies(X_train[categorical_features], drop_first=True, dtype=float)\n",
        "    X_val_cat   = pd.get_dummies(X_val[categorical_features],   drop_first=True, dtype=float)\n",
        "    X_test_cat  = pd.get_dummies(X_test[categorical_features],  drop_first=True, dtype=float)\n",
        "    train_cols  = X_train_cat.columns\n",
        "    X_val_cat   = X_val_cat.reindex(columns=train_cols,  fill_value=0.0)\n",
        "    X_test_cat  = X_test_cat.reindex(columns=train_cols, fill_value=0.0)\n",
        "else:\n",
        "    X_train_cat = pd.DataFrame(index=X_train.index)\n",
        "    X_val_cat   = pd.DataFrame(index=X_val.index)\n",
        "    X_test_cat  = pd.DataFrame(index=X_test.index)\n",
        "\n",
        "print(f\"[One-Hot] Train categorical columns: {X_train_cat.shape[1]}\")"
      ],
      "metadata": {
        "id": "8nIazfxJOF39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO a(ii) calculate z-score numerics\n",
        "def zscore_transform(X, mean, std):\n",
        "    \"\"\"\n",
        "    Apply z-score normalization to X using given mean and std.\n",
        "    Z-score is computed as: z = (x - mean) / std\n",
        "    \"\"\"\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "# Train stats\n",
        "train_mean = X_train[numerical_features].mean()\n",
        "train_std  = X_train[numerical_features].std(ddof=0).replace(0, 1)\n",
        "\n",
        "# Apply transform\n",
        "X_train_num = zscore_transform(X_train[numerical_features], train_mean, train_std)\n",
        "X_val_num   = zscore_transform(X_val[numerical_features],   train_mean, train_std)\n",
        "X_test_num  = zscore_transform(X_test[numerical_features],  train_mean, train_std)\n"
      ],
      "metadata": {
        "id": "bW7aucb9OJBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine\n",
        "X_train_processed = pd.concat([X_train_num, X_train_cat], axis=1).astype(np.float64)\n",
        "X_val_processed   = pd.concat([X_val_num,   X_val_cat],   axis=1).astype(np.float64)\n",
        "X_test_processed  = pd.concat([X_test_num,  X_test_cat],  axis=1).astype(np.float64)\n",
        "\n",
        "final_feature_names = X_train_processed.columns.tolist()\n",
        "print(f\"[Processed] Train design shape: {X_train_processed.shape}\")"
      ],
      "metadata": {
        "id": "_6klj-2WOF1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bias + numpy\n",
        "X_train_np = X_train_processed.to_numpy(dtype=np.float64)\n",
        "X_val_np   = X_val_processed.to_numpy(dtype=np.float64)\n",
        "X_test_np  = X_test_processed.to_numpy(dtype=np.float64)\n",
        "\n",
        "X_train_b = np.c_[np.ones((X_train_np.shape[0], 1)), X_train_np]\n",
        "X_val_b   = np.c_[np.ones((X_val_np.shape[0],   1)), X_val_np]\n",
        "X_test_b  = np.c_[np.ones((X_test_np.shape[0],  1)), X_test_np]\n",
        "\n",
        "print(f\"[Bias] X_train_b shape: {X_train_b.shape} (includes bias)\")"
      ],
      "metadata": {
        "id": "J3yKR0mKONW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Utilities\n",
        "# ------------------------------------------------------\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))"
      ],
      "metadata": {
        "id": "O54hNwkfNBNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Part 2: OLS and Ridge (Closed-Form)\n",
        "# ------------------------------------------------------"
      ],
      "metadata": {
        "id": "GkZLX2mmNBKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ols_solve_predict(X_train_b, y_train, X_test_b):\n",
        "    \"\"\"\n",
        "    Solve Ordinary Least Squares (OLS) using closed-form solution\n",
        "    and return weights and predictions on the test set.\n",
        "\n",
        "    Args:\n",
        "        X_train_b : Training design matrix (with bias column)\n",
        "        y_train   : Training targets\n",
        "        X_test_b  : Test design matrix (with bias column)\n",
        "\n",
        "    Returns:\n",
        "        w_ols     : OLS weight vector\n",
        "        y_pred    : Predictions on X_test_b\n",
        "    \"\"\"\n",
        "    #TODO a(iii) Implement OLS. You can use np.linalg.pinv for this\n",
        "\n",
        "    return w_ols, y_pred\n"
      ],
      "metadata": {
        "id": "KUURzPG8OWqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_ols, y_pred_ols = ols_solve_predict(X_train_b, y_train, X_test_b)\n",
        "mse_ols = mean_squared_error(y_test, y_pred_ols)\n",
        "rmse_ols = rmse(y_test, y_pred_ols)\n",
        "print(f\"[OLS] Test MSE: {mse_ols:.4f} | RMSE ($): {rmse_ols:,.2f}\")"
      ],
      "metadata": {
        "id": "hwnNUc4zlQEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#TODO b(i) Implement closed-form Ridge regression\n",
        "\n",
        "#TODO b(i)  Define I, XtX and Xty. Remember to set I[0, 0] to 0\n",
        "\n",
        "def ridge_precompute(X_train_b, y_train):\n",
        "    \"\"\"\n",
        "    Compute matrices for closed-form ridge regression.\n",
        "    Returns: I, XtX, Xty\n",
        "    \"\"\"\n",
        "\n",
        "    return I, XtX, Xty\n",
        "\n",
        "#TODO b(i) Calculate w_ridge and y_pred_ridge\n",
        "def ridge_solve_predict(XtX, Xty, I, best_lambda, X_test_b):\n",
        "    \"\"\"\n",
        "    Solve ridge closed-form weights and return predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    return w_ridge, y_pred_ridge"
      ],
      "metadata": {
        "id": "VqQADuSHfDKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "I, XtX, Xty = ridge_precompute(X_train_b, y_train)\n",
        "\n",
        "ridge_lambdas = [0.1, 1, 10, 100, 500, 1000]\n",
        "train_mses_ridge, val_mses_ridge = [], []\n",
        "\n",
        "for l in ridge_lambdas:\n",
        "    # Use the function to compute weights and predictions on train/val\n",
        "    w, y_train_pred = ridge_solve_predict(XtX, Xty, I, l, X_train_b)\n",
        "    _, y_val_pred   = ridge_solve_predict(XtX, Xty, I, l, X_val_b)\n",
        "\n",
        "    train_mses_ridge.append(mean_squared_error(y_train, y_train_pred))\n",
        "    val_mses_ridge.append(mean_squared_error(y_val,   y_val_pred))\n",
        "\n",
        "best_idx = int(np.argmin(val_mses_ridge))\n",
        "best_lambda_ridge = ridge_lambdas[best_idx]\n",
        "\n",
        "# Solve and predict\n",
        "w_ridge, y_pred_ridge = ridge_solve_predict(XtX, Xty, I, best_lambda_ridge, X_test_b)\n",
        "\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "rmse_ridge = rmse(y_test, y_pred_ridge)\n",
        "\n",
        "print(f\"[Ridge-Closed] Best λ={best_lambda_ridge}\")\n",
        "print(f\"[Ridge-Closed] Test MSE: {mse_ridge:.4f} | RMSE ($): {rmse_ridge:,.2f}\")\n"
      ],
      "metadata": {
        "id": "aktFpWlFOY8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SHOW_PLOTS:\n",
        "    plt.figure()\n",
        "    plt.plot(ridge_lambdas, train_mses_ridge, 'o-', label='Train MSE')\n",
        "    plt.plot(ridge_lambdas, val_mses_ridge,   'o-', label='Val MSE')\n",
        "    plt.xscale('log'); plt.xlabel('λ (Ridge)'); plt.ylabel('MSE'); plt.grid(True, ls='--')\n",
        "    plt.title('Ridge: Train/Val MSE vs λ'); plt.axvline(best_lambda_ridge, linestyle='--')\n",
        "    plt.legend(); plt.show()"
      ],
      "metadata": {
        "id": "faa1ysLBOY6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Part 3: Lasso via ISTA (Prox-Gradient)\n",
        "# ------------------------------------------------------"
      ],
      "metadata": {
        "id": "H-jupJpbOc2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lipschitz constant for data term (needed for safe step in ISTA)\n",
        "def lipschitz_constant_data(X):\n",
        "    d = X.shape[1]\n",
        "    v = np.random.randn(d); v /= (np.linalg.norm(v) + 1e-12)\n",
        "    for _ in range(100):\n",
        "        v = X.T @ (X @ v)\n",
        "        v /= (np.linalg.norm(v) + 1e-12)\n",
        "    lam_max = float(v @ (X.T @ (X @ v)))\n",
        "    return (2.0 / X.shape[0]) * lam_max\n",
        "\n",
        "L_data = lipschitz_constant_data(X_train_b)"
      ],
      "metadata": {
        "id": "Uyz8xcoVOgYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#TODO c(i) # set grad to the gradient of (1/n) * ||Xw - y||^2\n",
        "def compute_gradient(X, y, w):\n",
        "    \"\"\"\n",
        "    Compute the gradient of (1/n) * ||Xw - y||^2.\n",
        "    \"\"\"\n",
        "\n",
        "    return grad\n",
        "\n",
        "#TODO c(i) implement early stopping\n",
        "# if ||w - w_prev||_inf < tol, break the loop\n",
        "def check_early_stop(w, w_prev, tol):\n",
        "    \"\"\"\n",
        "    Check if ||w - w_prev||_inf < tol.\n",
        "    Returns True if early stopping should trigger.\n",
        "    \"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "def ista_lasso(X, y, alpha, n_iterations, lambda_val, tol=1e-8):\n",
        "    \"\"\"\n",
        "    ISTA for: (1/n)||Xw - y||^2 + lambda * ||w_{1:}||_1  (bias unpenalized).\n",
        "    Early-stops when ||w - w_prev||_inf < tol.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    w = np.zeros(d, dtype=np.float64)\n",
        "    thr = alpha * lambda_val\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        w_prev = w.copy()\n",
        "\n",
        "        # gradient step\n",
        "        grad = compute_gradient(X, y, w)\n",
        "        w -= alpha * grad\n",
        "\n",
        "        # soft-thresholding (skip bias term)\n",
        "        z = w[1:].copy()\n",
        "        w[1:] = np.sign(z) * np.maximum(np.abs(z) - thr, 0.0)\n",
        "\n",
        "        # early stopping\n",
        "        if check_early_stop(w, w_prev, tol):\n",
        "            break\n",
        "\n",
        "    return w"
      ],
      "metadata": {
        "id": "W54F6VUQNBHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune λ on a wide grid (linear target)\n",
        "alpha_lasso = 0.9 / L_data\n",
        "lasso_iters = 6000\n",
        "lasso_lambdas = np.logspace(1, 6, num=10)  # 1e1 ... 1e6\n",
        "\n",
        "lasso_val_mses, lasso_ws = [], []\n",
        "for l in lasso_lambdas:\n",
        "    w = ista_lasso(X_train_b, y_train, alpha_lasso, lasso_iters, l)\n",
        "    lasso_ws.append(w)\n",
        "    lasso_val_mses.append(mean_squared_error(y_val, X_val_b @ w))\n",
        "\n",
        "best_idx_lasso = int(np.argmin(lasso_val_mses))\n",
        "best_lambda_lasso = float(lasso_lambdas[best_idx_lasso])\n",
        "w_lasso = lasso_ws[best_idx_lasso]\n",
        "mse_lasso = mean_squared_error(y_test, X_test_b @ w_lasso)\n",
        "rmse_lasso = rmse(y_test, X_test_b @ w_lasso)\n",
        "\n",
        "# Sparsity (exclude bias)\n",
        "nonzeros_ols   = int(np.sum(np.abs(w_ols[1:])   > 1e-12))\n",
        "nonzeros_ridge = int(np.sum(np.abs(w_ridge[1:]) > 1e-12))\n",
        "nonzeros_lasso = int(np.sum(np.abs(w_lasso[1:]) > 1e-12))\n",
        "total_features = X_train_np.shape[1]\n",
        "\n",
        "print(f\"[Lasso-ISTA] Best λ={best_lambda_lasso:.2e} | Test MSE: {mse_lasso:.4f} | RMSE ($): {rmse_lasso:,.2f}\")\n",
        "print(f\"[Sparsity] OLS={nonzeros_ols}/{total_features}, Ridge={nonzeros_ridge}/{total_features}, Lasso={nonzeros_lasso}/{total_features}\")"
      ],
      "metadata": {
        "id": "kYQWLgFlhFOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = ['Bias'] + final_feature_names\n",
        "weights_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'OLS_Weights': w_ols,\n",
        "    'Ridge_Weights (best λ)': w_ridge,\n",
        "    'Lasso_Weights (best λ)': w_lasso\n",
        "})\n",
        "top10 = weights_df.reindex(\n",
        "    weights_df['Lasso_Weights (best λ)'].abs().sort_values(ascending=False).index\n",
        ").head(10).round(3)\n",
        "print(\"\\n[Top-10 |Lasso weights|]\\n\", top10)"
      ],
      "metadata": {
        "id": "5FJKJnw9OkA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Motivation: Target Skewness (before log-transform)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "y_train_series = pd.Series(y_train)\n",
        "#TODO d(i) log transform the y_train_series. You can use np.log1p for this.\n",
        "y_train_log =\n",
        "\n",
        "skew_raw = float(y_train_series.skew())\n",
        "skew_log = float(y_train_log.skew())\n",
        "print(f\"[Skew] SalePrice: {skew_raw:.3f} | log1p(SalePrice): {skew_log:.3f}\")\n",
        "\n",
        "if SHOW_PLOTS:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(y_train, bins=40)\n",
        "    plt.title(f\"SalePrice (train) — skew={skew_raw:.2f}\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(np.log1p(y_train), bins=40)\n",
        "    plt.title(f\"log1p(SalePrice) (train) — skew={skew_log:.2f}\")\n",
        "    plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "VIC65xL7NBEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO d(ii) calculate smear_ridge and y_pred_ridge_log_back\n",
        "# for smear_ridge you have to use np.exp on log residuals on train\n",
        "# for y_pred_ridge_log_back, you can use np.expm1 (https://numpy.org/doc/2.3/reference/generated/numpy.expm1.html)\n",
        "\n",
        "def smear_back_transform(y_train_log, train_pred_log, X_test_b, w):\n",
        "    \"\"\"\n",
        "    Apply Duan's smearing estimator to back-transform predictions.\n",
        "\n",
        "    Args:\n",
        "        y_train_log : log-transformed training targets\n",
        "        train_pred_log : log predictions on training set\n",
        "        X_test_b : test features (with bias column)\n",
        "        w : fitted weight vector\n",
        "\n",
        "    Returns:\n",
        "        smear : smearing correction factor\n",
        "        y_pred_back : back-transformed predictions on test set\n",
        "    \"\"\"\n",
        "\n",
        "    return smear, y_pred_back"
      ],
      "metadata": {
        "id": "SsMbj1wjkE_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Part 4: Log-Transform + Duan's Smearing (Ridge & Lasso)\n",
        "# ------------------------------------------------------\n",
        "print(\"\\n------------------------------------------------------\")\n",
        "print(\"Part 4: Log-Transform + Duan's Smearing (Ridge & Lasso)\")\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "#TODO d(ii) log-transform y_train, y_val and y_test\n",
        "# name the new variables y_train_log, y_val_log and y_test_log\n",
        "#y_train_log =\n",
        "#y_val_log   =\n",
        "#y_test_log  =\n",
        "\n",
        "# Precompute for log target (Xty depends on y)\n",
        "I, XtX, Xty_log = ridge_precompute(X_train_b, y_train_log)\n",
        "\n",
        "# Ridge on log target using helper\n",
        "val_mses_log_ridge, ridge_ws_log = [], []\n",
        "for l in ridge_lambdas:\n",
        "    #TODO d(ii) Calculate w_ridge and y_val_ridge\n",
        "    w_ridge_log_l, y_val_ridge = ridge_solve_predict(XtX, Xty_log, I, l, X_val_b)\n",
        "    ridge_ws_log.append(w_ridge_log_l)\n",
        "    val_mses_log_ridge.append(mean_squared_error(y_val_log, y_val_ridge))\n",
        "\n",
        "best_idx_ridge_log = int(np.argmin(val_mses_log_ridge))\n",
        "best_lambda_ridge_log = ridge_lambdas[best_idx_ridge_log]\n",
        "w_ridge_log = ridge_ws_log[best_idx_ridge_log]\n",
        "\n",
        "# Duan's smearing (log→linear)\n",
        "train_pred_log_ridge = X_train_b @ w_ridge_log\n",
        "smear_ridge, y_pred_ridge_log_back = smear_back_transform(\n",
        "    y_train_log, train_pred_log_ridge, X_test_b, w_ridge_log\n",
        ")\n",
        "\n",
        "mse_ridge_log_back  = mean_squared_error(y_test, y_pred_ridge_log_back)\n",
        "rmse_ridge_log_back = rmse(y_test, y_pred_ridge_log_back)\n",
        "\n",
        "# ISTA-Lasso on log target (unchanged)\n",
        "lasso_lambdas_log = np.logspace(-3, 2, num=16)  # 1e-3 ... 1e2\n",
        "lasso_iters_log = 20_000\n",
        "lasso_val_mses_log, lasso_ws_log = [], []\n",
        "for l in lasso_lambdas_log:\n",
        "    w = ista_lasso(X_train_b, y_train_log, alpha_lasso, lasso_iters_log, l)\n",
        "    lasso_ws_log.append(w)\n",
        "    lasso_val_mses_log.append(mean_squared_error(y_val_log, X_val_b @ w))\n",
        "\n",
        "best_idx_lasso_log = int(np.argmin(lasso_val_mses_log))\n",
        "best_lambda_lasso_log = float(lasso_lambdas_log[best_idx_lasso_log])\n",
        "w_lasso_log = lasso_ws_log[best_idx_lasso_log]\n",
        "\n",
        "train_pred_log_lasso = X_train_b @ w_lasso_log\n",
        "smear_lasso, y_pred_lasso_log_back = smear_back_transform(\n",
        "    y_train_log, train_pred_log_lasso, X_test_b, w_lasso_log\n",
        ")\n",
        "mse_lasso_log_back  = mean_squared_error(y_test, y_pred_lasso_log_back)\n",
        "rmse_lasso_log_back = rmse(y_test, y_pred_lasso_log_back)\n",
        "\n",
        "print(f\"[Ridge-LOG] Best λ={best_lambda_ridge_log} | MSE: {mse_ridge_log_back:.4f} | RMSE ($): {rmse_ridge_log_back:,.2f}\")\n",
        "print(f\"[Lasso-LOG] Best λ={best_lambda_lasso_log:.2e} | MSE: {mse_lasso_log_back:.4f} | RMSE ($): {rmse_lasso_log_back:,.2f}\")\n"
      ],
      "metadata": {
        "id": "udn_DCwWjf_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n==================== RESULTS SUMMARY ====================\")\n",
        "summary = pd.DataFrame({\n",
        "    \"Model\": [\n",
        "        \"OLS (MLE)\",\n",
        "        f\"Ridge (closed, λ={best_lambda_ridge})\",\n",
        "        f\"Lasso (ISTA, λ≈{best_lambda_lasso:.1e})\",\n",
        "        f\"Ridge (log→back, λ={best_lambda_ridge_log})\",\n",
        "        f\"Lasso (log→back, λ≈{best_lambda_lasso_log:.1e})\",\n",
        "    ],\n",
        "    \"Test MSE\": [\n",
        "        f\"{mse_ols:.4f}\",\n",
        "        f\"{mse_ridge:.4f}\",\n",
        "        f\"{mse_lasso:.4f}\",\n",
        "        f\"{mse_ridge_log_back:.4f}\",\n",
        "        f\"{mse_lasso_log_back:.4f}\",\n",
        "    ],\n",
        "    \"Test RMSE ($)\": [\n",
        "        f\"{rmse_ols:,.2f}\",\n",
        "        f\"{rmse_ridge:,.2f}\",\n",
        "        f\"{rmse_lasso:,.2f}\",\n",
        "        f\"{rmse_ridge_log_back:,.2f}\",\n",
        "        f\"{rmse_lasso_log_back:,.2f}\",\n",
        "    ]\n",
        "})\n",
        "print(summary.to_string(index=False))\n",
        "print(\"========================================================\")"
      ],
      "metadata": {
        "id": "jKYsmdLnNA-y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}