{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd2319f",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import load_digits, make_moons\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from hw6_q1 import autoencode\n",
    "from hw6_q3 import hw6_q3_autograd as ad\n",
    "from hw6_q3 import hw6_q3_nn as nn\n",
    "from hw6_q3 import hw6_q3_optim as optim\n",
    "from hw6_q3_utils import compute_num_params, train\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dee9ad",
   "metadata": {},
   "source": [
    "# Linear AE recovers PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4a192",
   "metadata": {},
   "source": [
    "We will first visualize PCA components on a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e00483",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_dataset = load_digits()\n",
    "print(f\"There are {digits_dataset.data.shape[0]} samples with {digits_dataset.data.shape[1]} features.\")\n",
    "\n",
    "def plot(data: np.ndarray, labels: np.ndarray, title: str):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\"tab10\", alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Digit Label\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.xlim(-30, 30)\n",
    "    plt.ylim(-30, 30)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "pca_components = pca.fit_transform(digits_dataset.data)\n",
    "\n",
    "plot(pca_components, digits_dataset.target, \"PCA of Digits Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e190c5",
   "metadata": {},
   "source": [
    "We will then run your implementation of the linear autoencoder. Since the AE will learn the same subspace as the PCA, we will have some similar-looking but not exactly the same components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349181e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "ae_components = autoencode(torch.tensor(digits_dataset.data)).numpy()\n",
    "plot(ae_components, digits_dataset.target, \"Autoencoder of Digits Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2a776",
   "metadata": {},
   "source": [
    "To find the exact transformation from our learned components to PCA components, we can perform a simple least squares. The autoencoder visualization should now match the PCA one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = np.linalg.lstsq(ae_components, pca_components)[0]\n",
    "print(f\"Learned linear transformation from AE space to PCA space:\\n{transformation}\")\n",
    "transformed_ae_components = ae_components @ transformation\n",
    "plot(transformed_ae_components, digits_dataset.target, \"Transformed Autoencoder of Digits Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e059f24",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01437e2a",
   "metadata": {},
   "source": [
    "We test the implementation of our automatic differentiation on the same written problem. The output below should match your manually derived results (with very small numerical differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "x0 = [0, 1]\n",
    "y0 = 1\n",
    "x1 = [1, 0]\n",
    "y1 = 0\n",
    "ad\n",
    "w = [ad.Scalar(1), ad.Scalar(1), ad.Scalar(1)]\n",
    "h = [ad.Scalar(1), ad.Scalar(1)]\n",
    "\n",
    "\n",
    "def forward_pass(x, y):\n",
    "    wx = sum(wi * xi for wi, xi in zip(w, [1] + x))\n",
    "    w_out = ad.ReLUFn().forward(wx)\n",
    "\n",
    "    y_hat = sum(hi * ui for hi, ui in zip(h, [1, w_out]))\n",
    "\n",
    "    loss = (y_hat - y) ** 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def backward_pass(loss, lr=0.05):\n",
    "    for wi in w:\n",
    "        wi.grad = 0\n",
    "    for hi in h:\n",
    "        hi.grad = 0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for wi in w:\n",
    "        wi.data -= wi.grad * lr\n",
    "    for hi in h:\n",
    "        hi.data -= hi.grad * lr\n",
    "\n",
    "\n",
    "loss0 = forward_pass(x0, y0)\n",
    "backward_pass(loss0)\n",
    "\n",
    "print(\"First training step\")\n",
    "print(\"Layer 1: w\")\n",
    "pprint(w)\n",
    "print(\"Layer 2: h\")\n",
    "pprint(h)\n",
    "\n",
    "loss1 = forward_pass(x1, y1)\n",
    "backward_pass(loss1)\n",
    "\n",
    "print(\"Second training step\")\n",
    "print(\"Layer 1: w\")\n",
    "pprint(w)\n",
    "print(\"Layer 2: h\")\n",
    "pprint(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251ae81",
   "metadata": {},
   "source": [
    "We provide a simple test on your binary cross-entropy implementation. The cell below should run successfully without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_test(i):\n",
    "    a1 = ad.Scalar(i)\n",
    "    a2 = torch.tensor([i * 1.0], requires_grad=True)\n",
    "\n",
    "    def impl_bce_with_logits(a):\n",
    "        return ad.BCEWithLogitsLossFn().forward(a, 1)\n",
    "\n",
    "    def torch_bce_with_logits(a):\n",
    "        return torch.nn.functional.binary_cross_entropy_with_logits(a, torch.Tensor([1]))\n",
    "\n",
    "    b1 = impl_bce_with_logits(a1)\n",
    "    b2 = torch_bce_with_logits(a2)\n",
    "    print(f\"For i = {i}\")\n",
    "\n",
    "    b1.backward()\n",
    "    b2.backward()\n",
    "    print(\"- Gradient\", b1.item(), b2.item())\n",
    "\n",
    "    assert torch.isclose(torch.tensor(b1.item()), torch.tensor(b2.item()))\n",
    "    assert torch.isclose(torch.tensor(a1.grad), torch.tensor(a2.grad.item()))\n",
    "\n",
    "\n",
    "for i in [-100, -50, -5, -1, 1, 5, 50, 100]:\n",
    "    bce_test(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253448e",
   "metadata": {},
   "source": [
    "We will now build a simple MLP to test our neural network framework. First, we load a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3714f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.1, random_state=0)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"jet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863880d",
   "metadata": {},
   "source": [
    "We will then define our version of MLP and PyTorch-based MLP. Our parameters are already initialized to $0.1$. Therefore, we would also need to initialize Pytorch MLP to $0.1$ for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208420d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        outputs = self.relu(self.linear1(inputs))\n",
    "        outputs = self.relu(self.linear2(outputs))\n",
    "        return self.linear3(outputs)\n",
    "\n",
    "\n",
    "class TorchMLP(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        for linear in [self.linear1, self.linear2, self.linear3]:\n",
    "            torch.nn.init.constant_(linear.weight, 0.1)\n",
    "            torch.nn.init.constant_(linear.bias, 0.1)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        outputs = self.relu(self.linear1(inputs))\n",
    "        outputs = self.relu(self.linear2(outputs))\n",
    "        return self.linear3(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de684c03",
   "metadata": {},
   "source": [
    "We will run our implementation. The final result should be about $0.89$. The training should take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_mlp = OurMLP(2, 16, 1)\n",
    "print(\"OurMLP trainable parameters:\", compute_num_params(our_mlp.parameters()))\n",
    "our_loss_fn = nn.BCEWithLogitsLoss()\n",
    "our_optimizer = optim.SGD(our_mlp.parameters(), lr=0.5)\n",
    "our_X = X\n",
    "our_y = y\n",
    "\n",
    "our_losses, out_accs = train(\n",
    "    mlp=our_mlp,\n",
    "    loss_fn=our_loss_fn,\n",
    "    optimizer=our_optimizer,\n",
    "    X=our_X,\n",
    "    y=our_y,\n",
    "    batch_size=8,\n",
    "    num_steps=50,\n",
    ")\n",
    "\n",
    "plt.subplots(1, 2, figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(our_losses)\n",
    "plt.title(\"Our MLP Training Loss\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(out_accs)\n",
    "plt.title(\"Our MLP Training Accuracy\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e4f74",
   "metadata": {},
   "source": [
    "We will now run the PyTorch implementation. Your implementation should match the PyTorch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_mlp = TorchMLP(2, 16, 1)\n",
    "print(\"TorchMLP trainable parameters:\", compute_num_params(torch_mlp.parameters()))\n",
    "torch_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "torch_optimizer = torch.optim.SGD(torch_mlp.parameters(), lr=0.5)\n",
    "torch_X = torch.Tensor(X)\n",
    "torch_y = torch.Tensor(y)\n",
    "\n",
    "torch_losses, torch_accs = train(\n",
    "    mlp=torch_mlp,\n",
    "    loss_fn=torch_loss_fn,\n",
    "    optimizer=torch_optimizer,\n",
    "    X=torch_X,\n",
    "    y=torch_y,\n",
    "    batch_size=8,\n",
    "    num_steps=50,\n",
    ")\n",
    "\n",
    "plt.subplots(1, 2, figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(torch_losses)\n",
    "plt.title(\"Torch MLP Training Loss\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(torch_accs)\n",
    "plt.title(\"Torch MLP Training Accuracy\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
